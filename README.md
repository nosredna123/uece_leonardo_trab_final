# Trabalho Final - Aprendizado de M√°quina e Minera√ß√£o de Dados

**Disciplina:** Aprendizado de M√°quina e Minera√ß√£o de Dados 2025.2  
**Professor:** Leonardo Rocha

---

## üöÄ Quick Start Guide

### Initial Setup

1. **Run the setup script:**
   ```bash
   bash setup.sh
   ```
   This will:
   - Create a Python virtual environment in `.venv/`
   - Install all dependencies from `requirements.txt`

2. **Activate the virtual environment:**
   ```bash
   source .venv/bin/activate
   ```

### üìÇ Dataset

The project includes the **GTFSBHTRANS** (BH Trans GTFS) dataset, which is already located in `data/raw/GTFSBHTRANS.zip`. This dataset contains public transportation data for Belo Horizonte's transit system.

**Converting to Parquet:**

To convert the GTFS txt files to Parquet format (more efficient for processing):

```bash
python src/data/convert_to_parquet.py
```

This will extract and convert all txt files to Parquet format in `data/processed/gtfs/`.

**Using in your code:**

```python
from src.data.gtfs_loader import GTFSLoader

# Initialize loader
loader = GTFSLoader()

# Load all GTFS files
gtfs_data = loader.load_all_files()

# Or load a specific parquet file
df_stops = loader.load_parquet('stops')
df_routes = loader.load_parquet('routes')
```

### üìä Development Workflow

#### Option A: Automated Pipeline (Recommended) ‚ö°

Run the complete ML pipeline with a single command:

```bash
# Run with current configuration
./run_pipeline.sh
```

This executes the full 8-step pipeline:
1. Spatial grid generation (based on `cell_size_meters` in config)
2. Feature extraction from GTFS data
3. Label generation for transit coverage
4. Data preprocessing (train/val/test splits)
5. Model training (Logistic Regression, Random Forest, Gradient Boosting)
6. Model evaluation and metrics
7. Model export to ONNX format
8. Report and visualization generation

**Customizing the Pipeline:**

Before running, edit `config/model_config.yaml` to adjust:

```bash
nano config/model_config.yaml
```

Key parameters:
- `cell_size_meters`: Grid resolution (100, 150, 200, 250, 500...)
  - **150m recommended** - best balance between accuracy and performance
- `test_size`: Test set proportion (default: 0.15)
- `validation_size`: Validation set proportion (default: 0.15)
- Model hyperparameters (max_iter, n_estimators, learning_rate, etc.)

**Expected Results:**
- Execution time: 5-10 minutes (150m grids), 2-3 minutes (250m grids)
- Output: F1-score ~0.75-0.85 (150m), ~0.85-0.92 (250m)
- Generated files: models, reports, visualizations

For detailed documentation, see:
- `PIPELINE_USAGE.md` - Comprehensive usage guide
- `REGENERATION_GUIDE.md` - Step-by-step instructions
- `GRID_SIZE_SOLUTION.md` - Grid size selection guide

#### Option B: Manual Step-by-Step (Advanced)

For exploratory analysis or custom workflows:

**Step 1: Exploratory Data Analysis**
```bash
jupyter notebook notebooks/01_exploratory_analysis.ipynb
```
- Load and explore GTFS dataset
- Analyze transit coverage patterns
- Check data quality and distributions

**Step 2: Run Individual Pipeline Steps**
```bash
# Generate spatial grid
python -m src.data.grid_generator

# Extract features from GTFS data
python -m src.data.feature_extractor

# Generate labels
python -m src.data.label_generator

# Preprocess and split data
python -m src.data.preprocessing

# Train models
python -m src.models.train

# Evaluate models
python -m src.models.evaluator

# Export best model
python -m src.models.export

# Generate report
python generate_report.py
```

### ü§ñ Running the API

After running the pipeline (which exports the model automatically):

```bash
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
```

Access the interactive API documentation at: `http://localhost:8000/docs`

#### Example API Request

Predict transit coverage for a specific location in Belo Horizonte:

```bash
# Check if a location has good transit coverage
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"latitude": -19.9167, "longitude": -43.9345}'

# Response:
# {
#   "prediction": 1,
#   "label": "well_served",
#   "probability": 0.89,
#   "features": {...}
# }
```

### üìù Using Python Modules Directly

You can also use the modules directly in Python:

```python
from src.data.preprocessing import DataPreprocessor
from src.features.feature_engineering import FeatureEngineer
from src.models.train import ModelTrainer

# Preprocess data
preprocessor = DataPreprocessor()
df = preprocessor.load_data('data/raw/your_data.csv')

# Train models
trainer = ModelTrainer()
trainer.initialize_models()
trainer.train_all_models(X_train, y_train, X_test, y_test)
```

### üîß Common Issues

#### Missing Dependencies
If you get import errors, make sure you:
1. Activated the virtual environment: `source .venv/bin/activate`
2. Installed all requirements: `pip install -r requirements.txt`

#### Script Not Executable
```bash
chmod +x run_pipeline.sh
./run_pipeline.sh
```

#### Model Not Found in API
Make sure you:
1. Ran the pipeline: `./run_pipeline.sh`
2. Check model exists: `ls -lh models/transit_coverage/best_model.onnx`
3. Restarted the API server

#### Out of Memory During Pipeline Execution
Use a larger grid size to reduce memory usage:
```bash
# Edit config to use 250m or 300m grids
nano config/model_config.yaml  # Set cell_size_meters: 250
./run_pipeline.sh
```

#### Pipeline Results Look Suspicious (F1 = 1.00)
This indicates over-aggregation. Use smaller grids:
```bash
# Edit config to use 150m or 200m grids
nano config/model_config.yaml  # Set cell_size_meters: 150
./run_pipeline.sh
```

See `reports/data_leakage_diagnostic.md` for detailed analysis.

---

## üéØ Features

### Transit Coverage Classifier

**Status:** In Specification  
**Branch:** `1-transit-coverage-classifier`  
**Specification:** [specs/1-transit-coverage-classifier/spec.md](specs/1-transit-coverage-classifier/spec.md)

Binary classification model to identify well-served vs underserved regions in Belo Horizonte based on GTFS transit data. This feature supports urban planning decisions and equitable mobility policy analysis.

**Key Capabilities:**
- Geographic grid-based analysis of transit coverage
- Feature extraction from GTFS data (stops, routes, trip frequency)
- Binary classification: well-served (1) vs underserved (0)
- Model export to ONNX format
- API endpoint for real-time predictions

**Success Criteria:**
- F1-score ‚â• 0.70 on test set
- API response time < 200ms per prediction
- Coverage analysis for 90%+ of city area

See the [full specification](specs/1-transit-coverage-classifier/spec.md) for details.

---

### üéØ Next Steps

1. ‚úÖ Setup environment (done by `setup.sh`)
2. ‚úÖ Dataset ready in `data/raw/GTFSBHTRANS.zip`
3. ‚úÖ Convert GTFS to Parquet: `python src/data/convert_to_parquet.py`
4. ‚öôÔ∏è Configure pipeline: `nano config/model_config.yaml` (set `cell_size_meters: 150`)
5. üöÄ Run complete pipeline: `./run_pipeline.sh` (5-10 minutes)
6. üìä Review results: `cat reports/tables/model_comparison.csv`
7. üìà Check visualizations: `ls reports/figures/`
8. ü§ñ Start API and test predictions: `uvicorn src.api.main:app --port 8000`
9. üìù Read technical report: `reports/relatorio_tecnico.md`

**Optional:** Run exploratory analysis notebook for deeper insights:
```bash
jupyter notebook notebooks/01_exploratory_analysis.ipynb
```

### üí° Tips

- Use `git add .` and `git commit` regularly to save progress
- Document your findings in the notebook markdown cells
- Export multiple model formats for compatibility
- Test the API thoroughly before final submission

---

## üìã Descri√ß√£o do Projeto

[Descreva aqui o problema abordado e a solu√ß√£o desenvolvida]

Este projeto implementa um pipeline completo de Machine Learning para [descrever a tarefa], utilizando o dataset [nome do dataset]. O objetivo √© [descrever o objetivo principal].

## üóÇÔ∏è Estrutura do Reposit√≥rio

```
.
‚îú‚îÄ‚îÄ data/                    # Dados do projeto
‚îÇ   ‚îú‚îÄ‚îÄ raw/                # Dados brutos originais
‚îÇ   ‚îî‚îÄ‚îÄ processed/          # Dados processados
‚îú‚îÄ‚îÄ notebooks/              # Jupyter notebooks para an√°lise explorat√≥ria
‚îÇ   ‚îú‚îÄ‚îÄ 01_exploratory_analysis.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_engineering.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_model_training.ipynb
‚îú‚îÄ‚îÄ src/                    # C√≥digo fonte do projeto
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data/              # Scripts para tratamento de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py
‚îÇ   ‚îú‚îÄ‚îÄ features/          # Engenharia de features
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering.py
‚îÇ   ‚îú‚îÄ‚îÄ models/            # Treinamento e exporta√ß√£o de modelos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ export.py
‚îÇ   ‚îî‚îÄ‚îÄ api/               # API para servir modelos
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ models/                 # Modelos treinados exportados
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ tests/                  # Testes unit√°rios
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ .gitignore             # Arquivos ignorados pelo Git
‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ setup.sh              # Script de setup do ambiente
‚îú‚îÄ‚îÄ README.md             # Este arquivo
‚îî‚îÄ‚îÄ trab-final-leonardo.pdf  # Especifica√ß√£o do trabalho

```

## üöÄ Como Executar

### 1. Configura√ß√£o do Ambiente

Execute o script de setup para criar o ambiente virtual e instalar as depend√™ncias:

```bash
bash setup.sh
```

Ou manualmente:

```bash
# Criar ambiente virtual
python3 -m venv .venv

# Ativar ambiente virtual
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate     # Windows

# Instalar depend√™ncias
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Executar Notebooks

Com o ambiente ativado, inicie o Jupyter:

```bash
jupyter notebook
```

Navegue at√© a pasta `notebooks/` e execute os notebooks na ordem:
1. `01_exploratory_analysis.ipynb` - An√°lise explorat√≥ria dos dados
2. `02_feature_engineering.ipynb` - Engenharia de features
3. `03_model_training.ipynb` - Treinamento e valida√ß√£o de modelos

### 3. Treinar Modelos via Script

```bash
python src/models/train.py
```

### 4. Executar API de Model Serving

Inicie a API FastAPI:

```bash
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
```

Acesse a documenta√ß√£o interativa em: `http://localhost:8000/docs`

#### Exemplo de Requisi√ß√£o

```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"features": [value1, value2, value3, ...]}'
```

## üìä Dataset

**Nome:** GTFSBHTRANS - BH Trans GTFS Data  
**Fonte:** Dados de transporte p√∫blico de Belo Horizonte  
**Localiza√ß√£o:** `data/raw/GTFSBHTRANS.zip`  
**Formato:** GTFS (General Transit Feed Specification)  
**Tamanho:** ~213 MB (arquivo compactado)  
**Conte√∫do:** 
- `stop_times.txt` - Hor√°rios de paradas
- `stops.txt` - Informa√ß√µes de paradas
- `routes.txt` - Rotas de √¥nibus
- `trips.txt` - Viagens
- `shapes.txt` - Geometrias das rotas
- `calendar.txt` e `calendar_dates.txt` - Calend√°rios de opera√ß√£o
- Outros arquivos GTFS

**Tarefa:** [A ser definida - classifica√ß√£o, regress√£o, clustering, etc.]

## ü§ñ Modelos Implementados

- **Modelo 1:** [Nome] - [M√©tricas principais]
- **Modelo 2:** [Nome] - [M√©tricas principais]
- **Modelo 3:** [Nome] - [M√©tricas principais]

**Melhor Modelo:** [Nome e justificativa]

## üìà Resultados

[Incluir m√©tricas principais, gr√°ficos relevantes e an√°lise dos resultados]

| Modelo | Accuracy | Precision | Recall | F1-Score |
|--------|----------|-----------|--------|----------|
| Modelo 1 | 0.00 | 0.00 | 0.00 | 0.00 |
| Modelo 2 | 0.00 | 0.00 | 0.00 | 0.00 |
| Modelo 3 | 0.00 | 0.00 | 0.00 | 0.00 |

## üîß Tecnologias Utilizadas

- **Python 3.10+**
- **Scikit-learn** - Algoritmos de ML
- **Pandas/NumPy** - Manipula√ß√£o de dados
- **Matplotlib/Seaborn/Plotly** - Visualiza√ß√£o
- **FastAPI** - API REST
- **ONNX Runtime** - Model serving
- **Jupyter** - Notebooks interativos

## üìù Depend√™ncias

Todas as depend√™ncias est√£o listadas em `requirements.txt`. Principais bibliotecas:
- numpy, pandas, scipy
- scikit-learn, xgboost, lightgbm
- onnx, onnxruntime
- fastapi, uvicorn
- jupyter, notebook

## üë• Autor(es)

- [Seu Nome] - [Matr√≠cula]
- [Nome do Parceiro] - [Matr√≠cula] _(se aplic√°vel)_

## üìÑ Licen√ßa

Este projeto foi desenvolvido como trabalho acad√™mico para a disciplina de Aprendizado de M√°quina e Minera√ß√£o de Dados da UECE.

## üôè Agradecimentos

- Prof. Leonardo Rocha
- [Outras refer√™ncias ou agradecimentos]
