#!/bin/bash
# Transit Coverage Pipeline - Full Execution Script
# 
# This script runs the complete ML pipeline from scratch:
#   1. Spatial grid generation
#   2. Feature extraction
#   3. Label generation
#   4. Data preprocessing
#   5. Model training
#   6. Model evaluation
#   7. Model export (ONNX)
#   8. Report generation
# 
# Usage:
#   ./run_pipeline.sh
#
# Configuration:
#   Edit config/model_config.yaml before running to adjust:
#   - cell_size_meters (grid resolution: 100, 150, 200, 250, 500...)
#   - model hyperparameters
#   - train/val/test split ratios

set -e  # Exit on error

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  Transit Coverage Pipeline - Full Execution                  â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Step 0: Stop any running API/uvicorn processes
echo "ğŸ›‘ Stopping any running API processes..."
pkill -f "uvicorn.*src.api.main" 2>/dev/null || true
sleep 1
echo "âœ“ Running processes stopped"
echo ""

# Read current grid size from config
GRID_SIZE=$(grep "cell_size_meters:" config/model_config.yaml | awk '{print $2}')

if [ -z "$GRID_SIZE" ]; then
    echo "âŒ ERROR: Could not read cell_size_meters from config/model_config.yaml"
    exit 1
fi

echo "âœ“ Config verified: cell_size_meters = ${GRID_SIZE}m"
echo ""

# Calculate expected cells (approximate)
AREA_KM2=812  # Approximate coverage area for Belo Horizonte
CELL_AREA=$(echo "scale=4; ($GRID_SIZE / 1000) ^ 2" | bc)
EXPECTED_CELLS=$(echo "scale=0; $AREA_KM2 / $CELL_AREA" | bc)

echo "ğŸ“Š Grid Configuration:"
echo "   â€¢ Cell size: ${GRID_SIZE}m Ã— ${GRID_SIZE}m"
echo "   â€¢ Cell area: ${CELL_AREA} kmÂ²"
echo "   â€¢ Expected cells: ~$(printf "%'d" $EXPECTED_CELLS)"
echo ""

# Clean previous pipeline outputs
echo "ğŸ§¹ Cleaning previous pipeline outputs..."
echo "   â€¢ Removing grid data..."
rm -rf data/processed/grids/*
echo "   â€¢ Removing features..."
rm -rf data/processed/features/*
echo "   â€¢ Removing labels..."
rm -rf data/processed/labels/*
echo "   â€¢ Removing trained models (forces retraining)..."
rm -f models/transit_coverage/*.pkl
rm -f models/transit_coverage/*.onnx
rm -f models/transit_coverage/*.json
echo "   â€¢ Removing reports and visualizations..."
rm -rf reports/figures/*.png
rm -rf reports/tables/*.txt
rm -rf reports/tables/*.csv
echo "âœ“ All pipeline outputs cleaned (ready for fresh regeneration)"
echo ""
echo "ğŸš€ Starting pipeline execution..."
echo ""

# Initialize timing
PIPELINE_START=$(date +%s)
POPULATION_TIME=0

# Step 1: Generate grid
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "STEP 1/9: Generating ${GRID_SIZE}m Ã— ${GRID_SIZE}m spatial grid..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
STEP_START=$(date +%s)
python -m src.data.grid_generator
STEP_END=$(date +%s)
STEP_TIME=$((STEP_END - STEP_START))
echo "âœ“ Grid generated (${STEP_TIME}s)"
echo ""

# Step 2: Extract features
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "STEP 2/9: Extracting transit coverage features..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
STEP_START=$(date +%s)
python -m src.data.feature_extractor
STEP_END=$(date +%s)
STEP_TIME=$((STEP_END - STEP_START))
echo "âœ“ Features extracted (${STEP_TIME}s)"
echo ""

# Step 2.5: Integrate population data (NEW)
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "STEP 3/9: Integrating IBGE population data..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
STEP_START=$(date +%s)
# Check if IBGE data file exists before attempting integration
IBGE_ZIP="data/raw/ibge_populacao_bh_grade_id36.zip"
if [ -f "$IBGE_ZIP" ]; then
    echo "âœ“ IBGE data found: $IBGE_ZIP"
    python -m src.data.population_integrator
    # Rename output to replace the original features file
    if [ -f "data/processed/features/grid_features_with_population.parquet" ]; then
        mv data/processed/features/grid_features.parquet data/processed/features/grid_features_transit_only.parquet
        mv data/processed/features/grid_features_with_population.parquet data/processed/features/grid_features.parquet
        echo "âœ“ Population data integrated successfully"
        
        # Normalize population feature
        echo "  Normalizing population feature..."
        python -m src.data.normalize_population
        echo "âœ“ Population feature normalized"
    else
        echo "âš ï¸  Population integration failed - continuing with transit-only features"
    fi
else
    echo "âš ï¸  IBGE data not found at: $IBGE_ZIP"
    echo "    Skipping population integration (will use transit-only features)"
    echo "    To enable: Download IBGE data and place at $IBGE_ZIP"
fi
STEP_END=$(date +%s)
POPULATION_TIME=$((STEP_END - STEP_START))
echo "âœ“ Population integration completed (${POPULATION_TIME}s)"
if [ $POPULATION_TIME -gt 300 ]; then
    echo "âš ï¸  WARNING: Population integration exceeded 5-minute limit (FR-017)"
fi
echo ""

# Step 3: Generate labels
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "STEP 4/9: Generating binary labels..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
STEP_START=$(date +%s)
python -m src.data.label_generator
STEP_END=$(date +%s)
STEP_TIME=$((STEP_END - STEP_START))
echo "âœ“ Labels generated (${STEP_TIME}s)"
echo ""

# Step 4: Prepare splits
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "STEP 5/9: Creating train/val/test splits..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
STEP_START=$(date +%s)
python -m src.data.preprocessing
STEP_END=$(date +%s)
STEP_TIME=$((STEP_END - STEP_START))
echo "âœ“ Data splits created (${STEP_TIME}s)"
echo ""

# Step 5: Train models
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "STEP 6/9: Training models (LR, RF, GB)..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
STEP_START=$(date +%s)
python -m src.models.train
STEP_END=$(date +%s)
STEP_TIME=$((STEP_END - STEP_START))
echo "âœ“ Models trained (${STEP_TIME}s)"
echo ""

# Step 6: Evaluate models
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "STEP 7/9: Evaluating models and generating plots..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
STEP_START=$(date +%s)
python -m src.models.evaluator
STEP_END=$(date +%s)
STEP_TIME=$((STEP_END - STEP_START))
echo "âœ“ Evaluation complete (${STEP_TIME}s)"
echo ""

# Step 7: Export to ONNX
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "STEP 8/9: Exporting best model to ONNX..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
STEP_START=$(date +%s)
python -m src.models.export
STEP_END=$(date +%s)
STEP_TIME=$((STEP_END - STEP_START))
echo "âœ“ Model exported (${STEP_TIME}s)"
echo ""

# Step 8: Regenerate report
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "STEP 9/9: Regenerating technical report..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
STEP_START=$(date +%s)
python generate_report.py
STEP_END=$(date +%s)
STEP_TIME=$((STEP_END - STEP_START))
echo "âœ“ Report generated (${STEP_TIME}s)"
echo ""

# Calculate total execution time
PIPELINE_END=$(date +%s)
TOTAL_TIME=$((PIPELINE_END - PIPELINE_START))
TOTAL_MIN=$((TOTAL_TIME / 60))
TOTAL_SEC=$((TOTAL_TIME % 60))

# Display results summary
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  âœ… PIPELINE EXECUTION COMPLETE!                             â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "â±ï¸  Execution Time:"
echo "   Total:        ${TOTAL_MIN}m ${TOTAL_SEC}s"
echo "   Population:   ${POPULATION_TIME}s"
if [ $TOTAL_TIME -gt 600 ]; then
    echo "   âš ï¸  WARNING: Total execution exceeded 10-minute target (FR-017)"
fi
if [ $POPULATION_TIME -gt 300 ]; then
    echo "   âš ï¸  WARNING: Population integration exceeded 5-minute target (FR-017)"
fi
echo ""

# Show comparison
python -c "
import pandas as pd
import sys

print('ğŸ“Š RESULTS SUMMARY')
print('=' * 70)

# Read grid size from environment
grid_size = ${GRID_SIZE}

# New results
try:
    new = pd.read_csv('reports/tables/model_comparison.csv')
    
    # Find best model by F1 score
    best_idx = new['f1_score'].idxmax()
    best = new.iloc[best_idx]
    
    print(f'\n{grid_size}m Grid Results:')
    print(f'  Best Model: {best[\"model_name\"]}')
    print(f'  F1-Score:   {best[\"f1_score\"]:.4f}')
    print(f'  Accuracy:   {best[\"accuracy\"]:.4f}')
    
    # Provide interpretation based on F1 score
    f1 = float(best['f1_score'])
    if f1 >= 0.98:
        print('  Status:     âš ï¸  Very high - check for data leakage or over-aggregation')
    elif f1 >= 0.90:
        print('  Status:     âœ… Excellent performance')
    elif f1 >= 0.80:
        print('  Status:     âœ… Good performance - realistic for spatial classification')
    elif f1 >= 0.70:
        print('  Status:     âœ“ Acceptable performance')
    else:
        print('  Status:     âš ï¸  Low performance - consider larger grids or feature engineering')
    
    print('')
    print('All Models:')
    print('-' * 70)
    for idx, row in new.iterrows():
        print(f'  {row[\"model_name\"]:<20} F1={row[\"f1_score\"]:.4f}  Acc={row[\"accuracy\"]:.4f}')
    
    print('\n' + '=' * 70)
    print(f'âœ¨ Pipeline executed successfully with {grid_size}m spatial resolution!')
    print('=' * 70)
except Exception as e:
    print(f'âš ï¸  Could not read results: {e}')
    print('Check: reports/tables/model_comparison.csv')
    sys.exit(1)
"

echo ""
echo "ğŸ“ Generated Files:"
echo "   â€¢ data/processed/grids/cells.parquet"
echo "   â€¢ data/processed/features/*.parquet"
echo "   â€¢ models/transit_coverage/best_model.onnx"
echo "   â€¢ reports/figures/*.png"
echo "   â€¢ reports/tables/*.csv"
echo "   â€¢ reports/relatorio_tecnico.md"
echo ""
echo "ğŸ¯ Next Steps:"
echo "   1. Review results: cat reports/tables/model_comparison.csv"
echo "   2. Check visualizations: ls reports/figures/"
echo "   3. View report: head -100 reports/relatorio_tecnico.md"
echo ""
echo "ğŸš€ Starting API server with auto-reload..."
echo ""

# Kill any remaining processes on port 8000
lsof -i :8000 | grep LISTEN | awk '{print $2}' | xargs kill -9 2>/dev/null || true
sleep 1

# Start API with reload
echo "Starting uvicorn on http://localhost:8000"
echo "API Documentation: http://localhost:8000/docs"
echo ""
echo "Press Ctrl+C to stop the server"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
uvicorn src.api.main:app --reload --port 8000
